import onnxruntime as ort
import onnx
from onnx import helper as helper
from onnx import TensorProto as tp
from onnx.helper import make_opsetid
import numpy as np


def getNPType(dataType):
    if (dataType == tp.UINT32):
        return np.uint32
    elif (dataType == tp.INT32):
        return np.int32
    else:
        return np.float32


def buildAndRunBinaryGraph(op, DATA_TYPE, comment):
    NP_TYPE = getNPType(DATA_TYPE)

    t1 = np.array([4, 8, 9]).astype(NP_TYPE)
    t2 = np.array([1, 3, 9]).astype(NP_TYPE)

    inputShape = [1, 5, 8, 7, 1]
    input = np.array([0.0035714285714285713, 0.007142857142857143, 0.010714285714285714, 0.014285714285714285,
                      0.017857142857142856, 0.02142857142857143, 0.025, 0.02857142857142857, 0.03214285714285714,
                      0.03571428571428571, 0.039285714285714285, 0.04285714285714286, 0.04642857142857143, 0.05,
                      0.05357142857142857, 0.05714285714285714, 0.060714285714285714, 0.06428571428571428, 0.06785714285714285,
                      0.07142857142857142, 0.075, 0.07857142857142857, 0.08214285714285714, 0.08571428571428572,
                      0.08928571428571429, 0.09285714285714286, 0.09642857142857143, 0.1, 0.10357142857142858,
                      0.10714285714285714, 0.11071428571428571, 0.11428571428571428, 0.11785714285714285, 0.12142857142857143,
                      0.125, 0.12857142857142856, 0.13214285714285715, 0.1357142857142857, 0.1392857142857143,
                      0.14285714285714285, 0.14642857142857144, 0.15, 0.15357142857142858, 0.15714285714285714,
                      0.16071428571428573, 0.16428571428571428, 0.16785714285714284, 0.17142857142857143, 0.175,
                      0.17857142857142858, 0.18214285714285713, 0.18571428571428572, 0.18928571428571428, 0.19285714285714287,
                      0.19642857142857142, 0.2, 0.20357142857142857, 0.20714285714285716, 0.21071428571428572,
                      0.21428571428571427, 0.21785714285714286, 0.22142857142857142, 0.225, 0.22857142857142856,
                      0.23214285714285715, 0.2357142857142857, 0.2392857142857143, 0.24285714285714285, 0.24642857142857144,
                      0.25, 0.25357142857142856, 0.2571428571428571, 0.26071428571428573, 0.2642857142857143,
                      0.26785714285714285, 0.2714285714285714, 0.275, 0.2785714285714286, 0.28214285714285714,
                      0.2857142857142857, 0.2892857142857143, 0.29285714285714287, 0.29642857142857143, 0.3,
                      0.30357142857142855, 0.30714285714285716, 0.3107142857142857, 0.3142857142857143, 0.31785714285714284,
                      0.32142857142857145, 0.325, 0.32857142857142857, 0.33214285714285713, 0.3357142857142857,
                      0.3392857142857143, 0.34285714285714286, 0.3464285714285714, 0.35, 0.3535714285714286,
                      0.35714285714285715, 0.3607142857142857, 0.36428571428571427, 0.3678571428571429, 0.37142857142857144,
                      0.375, 0.37857142857142856, 0.3821428571428571, 0.38571428571428573, 0.3892857142857143,
                      0.39285714285714285, 0.3964285714285714, 0.4, 0.4035714285714286, 0.40714285714285714, 0.4107142857142857,
                      0.4142857142857143, 0.41785714285714287, 0.42142857142857143, 0.425, 0.42857142857142855,
                      0.43214285714285716, 0.4357142857142857, 0.4392857142857143, 0.44285714285714284, 0.44642857142857145,
                      0.45, 0.45357142857142857, 0.45714285714285713, 0.4607142857142857, 0.4642857142857143,
                      0.46785714285714286, 0.4714285714285714, 0.475, 0.4785714285714286, 0.48214285714285715,
                      0.4857142857142857, 0.48928571428571427, 0.4928571428571429, 0.49642857142857144, 0.5, 0.5035714285714286,
                      0.5071428571428571, 0.5107142857142857, 0.5142857142857142, 0.5178571428571429, 0.5214285714285715, 0.525,
                      0.5285714285714286, 0.5321428571428571, 0.5357142857142857, 0.5392857142857143, 0.5428571428571428,
                      0.5464285714285714, 0.55, 0.5535714285714286, 0.5571428571428572, 0.5607142857142857, 0.5642857142857143,
                      0.5678571428571428, 0.5714285714285714, 0.575, 0.5785714285714286, 0.5821428571428572, 0.5857142857142857,
                      0.5892857142857143, 0.5928571428571429, 0.5964285714285714, 0.6, 0.6035714285714285, 0.6071428571428571,
                      0.6107142857142858, 0.6142857142857143, 0.6178571428571429, 0.6214285714285714, 0.625, 0.6285714285714286,
                      0.6321428571428571, 0.6357142857142857, 0.6392857142857142, 0.6428571428571429, 0.6464285714285715, 0.65,
                      0.6535714285714286, 0.6571428571428571, 0.6607142857142857, 0.6642857142857143, 0.6678571428571428,
                      0.6714285714285714, 0.675, 0.6785714285714286, 0.6821428571428572, 0.6857142857142857, 0.6892857142857143,
                      0.6928571428571428, 0.6964285714285714, 0.7, 0.7035714285714286, 0.7071428571428572, 0.7107142857142857,
                      0.7142857142857143, 0.7178571428571429, 0.7214285714285714, 0.725, 0.7285714285714285, 0.7321428571428571,
                      0.7357142857142858, 0.7392857142857143, 0.7428571428571429, 0.7464285714285714, 0.75, 0.7535714285714286,
                      0.7571428571428571, 0.7607142857142857, 0.7642857142857142, 0.7678571428571429, 0.7714285714285715, 0.775,
                      0.7785714285714286, 0.7821428571428571, 0.7857142857142857, 0.7892857142857143, 0.7928571428571428,
                      0.7964285714285714, 0.8, 0.8035714285714286, 0.8071428571428572, 0.8107142857142857, 0.8142857142857143,
                      0.8178571428571428, 0.8214285714285714, 0.825, 0.8285714285714286, 0.8321428571428572, 0.8357142857142857,
                      0.8392857142857143, 0.8428571428571429, 0.8464285714285714, 0.85, 0.8535714285714285, 0.8571428571428571,
                      0.8607142857142858, 0.8642857142857143, 0.8678571428571429, 0.8714285714285714, 0.875, 0.8785714285714286,
                      0.8821428571428571, 0.8857142857142857, 0.8892857142857142, 0.8928571428571429, 0.8964285714285715, 0.9,
                      0.9035714285714286, 0.9071428571428571, 0.9107142857142857, 0.9142857142857143, 0.9178571428571428,
                      0.9214285714285714, 0.925, 0.9285714285714286, 0.9321428571428572, 0.9357142857142857, 0.9392857142857143,
                      0.9428571428571428, 0.9464285714285714, 0.95, 0.9535714285714286, 0.9571428571428572, 0.9607142857142857,
                      0.9642857142857143, 0.9678571428571429, 0.9714285714285714, 0.975, 0.9785714285714285, 0.9821428571428571,
                      0.9857142857142858, 0.9892857142857143, 0.9928571428571429, 0.9964285714285714, 1], dtype=np.float32).reshape(inputShape)
    # 1, 9, 1, 1, 1, 12, 21, 131, 131, 22, 21, 2, 2, 2, 2, 2, 22, 21, 2, 2, 2, 131, 22, 21
    # print("Raw Data from 1 to, BNSH: ")
    # print(*v.flatten(),sep=', ')
    # v = np.array([], dtype=np.float32).reshape(shape)
    filterShape = [1, 2, 3, 1, 1]
    filter = np.array([0.16666666666666666, 0.3333333333333333, 0.5, 0.6666666666666666,
                      0.8333333333333334, 1], dtype=np.float32).reshape(filterShape)
    biasShape = [1]
    bias = np.array([100], dtype=np.float32).reshape(biasShape)

    outShape = [1, 3, 3, 7, 1]
    # The required constants:

    # The functional nodes:
    '''
          { "name": "kernel_shape", "data": [1, 2, 3], "type": "ints" },
      { "name": "auto_pad", "data": "SAME_UPPER", "type": "string" },
      //{ "name": "pads", "data": [1,1,0,0,0,0], "type": "ints" },
      { "name": "strides", "data": [2, 3, 1], "type": "ints" },
      { "name": "dilations", "data": [1, 1, 1], "type": "ints" }
    '''
    n1 = helper.make_node(op, inputs=['X', 'W', 'B'], outputs=['Y'], name='Conv')
    kernel_shape_attr = helper.make_attribute("kernel_shape", [1,2,3])
    n1.attribute.append(kernel_shape_attr)
    auto_pad_attr = helper.make_attribute("auto_pad", "SAME_UPPER")
    n1.attribute.append(auto_pad_attr)
    strides_attr = helper.make_attribute("strides", [2, 3, 1])
    n1.attribute.append(strides_attr)
    dilations_attr = helper.make_attribute("dilations", [1,1,1])
    n1.attribute.append(dilations_attr)
    #channels_last
    #channels_last_attr = helper.make_attribute("channels_last", 1)
    #n1.attribute.append(channels_last_attr)
    #n1.attribute.extend([helper.make_attribute("channels_last", 1)])
    #n1.domain = "com.microsoft"
    n1.domain = "com.ms.internal.nhwc"
    # Create the graph
    g1 = helper.make_graph([n1], 'preprocessing',
                           [helper.make_tensor_value_info('X', DATA_TYPE, inputShape), helper.make_tensor_value_info(
                               'W', DATA_TYPE, filterShape), helper.make_tensor_value_info('B', DATA_TYPE, biasShape)],
                           [helper.make_tensor_value_info('Y', DATA_TYPE, outShape)])
    


    # Create the model and check
    m1 = helper.make_model(g1, producer_name='onnxsub-demo')
    # opset_imports=[make_opsetid("com.ms.internal.nhwc", 11)]
    m1.ir_version = 11
    # onnx.checker.check_model(m1)
    # Save the model
    MODEL_NAME = op+'_' + type(DATA_TYPE).__name__ + '_' + comment + '.onnx'
    onnx.save(m1, MODEL_NAME)
    ort_sess = ort.InferenceSession(MODEL_NAME)
    outputs = ort_sess.run(None, {'X': input, 'W': filter, 'B': bias})
    # Print Result
    print(type(DATA_TYPE).__name__, outputs[0])


op = 'Conv'
DATA_TYPE = tp.FLOAT
buildAndRunBinaryGraph(op, DATA_TYPE, 'FLOAT')
